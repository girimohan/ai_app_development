{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPwy+pNN57Ol9RZVTYgt9EH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d79Ng3UHNrpR","executionInfo":{"status":"ok","timestamp":1728459521665,"user_tz":-180,"elapsed":21783,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}},"outputId":"4f940324-40e2-4fe3-d1f6-5c3d4d3e5fe5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NtcOKUFuYO8"},"outputs":[],"source":["\n","# !pip install transformers torch librosa numpy pandas matplotlib\n"]},{"cell_type":"code","source":["# !pip install gradio"],"metadata":{"id":"Y0P8bkGbLiL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import librosa\n","import torch\n","import matplotlib.pyplot as plt\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, DistilBertTokenizer, DistilBertForSequenceClassification\n","import gradio as gr\n","import logging\n","import traceback\n"],"metadata":{"id":"yntMYQcEk3Xq","executionInfo":{"status":"ok","timestamp":1728552193023,"user_tz":-180,"elapsed":276,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n"],"metadata":{"id":"0tMj7Wlyk3UM","executionInfo":{"status":"ok","timestamp":1728552195337,"user_tz":-180,"elapsed":305,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Load models(fine-tuned)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","try:\n","    wav2vec2_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","    wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","    distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n","    distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english').to(device)\n","    logger.info(\"Models loaded successfully\")\n","except Exception as e:\n","    logger.error(f\"Error loading models: {str(e)}\")\n","    raise\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei_sEOKgk3N7","executionInfo":{"status":"ok","timestamp":1728552200973,"user_tz":-180,"elapsed":3876,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}},"outputId":"2c18a005-cb3f-45dc-9bac-0e2f1a600a06"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n","The class this function is called from is 'Wav2Vec2Tokenizer'.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n","- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Define audio processing functions\n","def speech_to_text(audio):\n","    try:\n","        input_values = wav2vec2_tokenizer(audio, return_tensors=\"pt\").input_values.to(device)\n","        logits = wav2vec2_model(input_values).logits\n","        predicted_ids = torch.argmax(logits, dim=-1)\n","        transcription = wav2vec2_tokenizer.batch_decode(predicted_ids)[0]\n","        return transcription\n","    except Exception as e:\n","        logger.error(f\"Error in speech_to_text: {str(e)}\")\n","        return f\"Error in speech_to_text: {str(e)}\\n{traceback.format_exc()}\"\n","\n","def analyze_sentiment(text):\n","    try:\n","        inputs = distilbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n","        outputs = distilbert_model(**inputs)\n","        sentiment = torch.argmax(outputs.logits, dim=1)\n","        sentiment_label = \"Positive\" if sentiment.item() == 1 else \"Negative\"\n","        return sentiment_label\n","    except Exception as e:\n","        logger.error(f\"Error in analyze_sentiment: {str(e)}\")\n","        return f\"Error in analyze_sentiment: {str(e)}\\n{traceback.format_exc()}\"\n","\n","def extract_audio_features(audio):\n","    try:\n","        # Calculate pitches\n","        pitches, _ = librosa.piptrack(y=audio, sr=22050)\n","        pitch_values = pitches[pitches > 0]  # Get non-zero pitch values\n","        pitch = np.mean(pitch_values) if pitch_values.size > 0 else 0  # Calculate mean pitch\n","\n","        # Calculate tempo\n","        tempo, _ = librosa.beat.beat_track(y=audio, sr=22050)\n","\n","        # Calculate spectral centroid (tone)\n","        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=22050)\n","        tone = np.mean(spectral_centroids) if spectral_centroids.size > 0 else 0\n","\n","        return float(pitch), float(tempo), float(tone)  # Ensure these are floats\n","    except Exception as e:\n","        logger.error(f\"Error in extract_audio_features: {str(e)}\")\n","        return 0.0, 0.0, 0.0  # Return zeros as floats on error\n","\n","\n","\n","\n","def process_audio(audio, sample_rate):\n","    try:\n","        logger.info(f\"Processing audio with sample rate: {sample_rate}\")\n","\n","        # Convert audio to float32 if it's not already\n","        if audio.dtype != np.float32:\n","            audio = audio.astype(np.float32)\n","\n","        # Normalize audio to be in the range [-1.0, 1.0]\n","        audio = audio / np.max(np.abs(audio))\n","\n","        # Resample audio to 16kHz for Wav2Vec2\n","        audio_16k = librosa.resample(y=audio, orig_sr=sample_rate, target_sr=16000)\n","\n","        # Speech to text\n","        transcription = speech_to_text(audio_16k)\n","        logger.info(f\"Transcription: {transcription}\")\n","\n","        # Sentiment analysis\n","        sentiment = analyze_sentiment(transcription)\n","        logger.info(f\"Sentiment: {sentiment}\")\n","\n","        # Extract audio features\n","        pitch, tempo, tone = extract_audio_features(audio_16k)\n","        logger.info(f\"Audio features - Pitch: {pitch}, Tempo: {tempo}, Tone: {tone}\")\n","\n","        return transcription, sentiment, pitch, tempo, tone\n","    except Exception as e:\n","        logger.error(f\"Error in process_audio: {str(e)}\")\n","        return f\"Error in process_audio: {str(e)}\\n{traceback.format_exc()}\", \"Error\", 0, 0, 0\n"],"metadata":{"id":"ELTrzBuhk3LH","executionInfo":{"status":"ok","timestamp":1728552200973,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Define visualization functions\n","def plot_waveform(audio):\n","    plt.figure(figsize=(10, 4))\n","    plt.plot(audio)\n","    plt.title('Audio Waveform')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Amplitude')\n","    plt.grid()\n","    plt.close()\n","    return plt\n","\n","def plot_features(pitch, tempo, tone):\n","    labels = ['Pitch', 'Tempo', 'Tone']\n","    values = [pitch, tempo, tone]\n","\n","    # Ensure values are all floats for the bar plot\n","    logger.info(f\"Plotting features with values: {values} (Types: {[type(v) for v in values]})\")\n","\n","    plt.figure(figsize=(10, 4))\n","    plt.bar(labels, values, color=['blue', 'orange', 'green'])\n","    plt.title('Audio Features')\n","    plt.ylabel('Value')\n","    plt.ylim(0, max(values) + 1)  # Set appropriate y-limits\n","    plt.close()\n","    return plt\n","\n","\n"],"metadata":{"id":"9QIItDXQk3H6","executionInfo":{"status":"ok","timestamp":1728552205414,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def audio_sentiment_analyzer(audio):\n","    if audio is None:\n","        logger.warning(\"No audio received\")\n","        return \"No audio recorded\", \"N/A\", None, None\n","\n","    try:\n","        sr, audio = audio\n","        logger.info(f\"Received audio with sample rate: {sr} and shape: {audio.shape}\")\n","\n","        transcription, sentiment, pitch, tempo, tone = process_audio(audio, sr)\n","\n","        # Debugging: Log pitch, tempo, tone types and values\n","        logger.info(f\"Pitch: {pitch} (Type: {type(pitch)})\")\n","        logger.info(f\"Tempo: {tempo} (Type: {type(tempo)})\")\n","        logger.info(f\"Tone: {tone} (Type: {type(tone)})\")\n","\n","        # Create visualizations\n","        fig_waveform = plot_waveform(audio)\n","        fig_features = plot_features(pitch, tempo, tone)\n","\n","        return transcription, sentiment, fig_waveform, fig_features\n","    except Exception as e:\n","        error_msg = f\"Error in audio_sentiment_analyzer: {str(e)}\\n{traceback.format_exc()}\"\n","        logger.error(error_msg)\n","        return error_msg, error_msg, None, None\n"],"metadata":{"id":"29kNRpdgk29c","executionInfo":{"status":"ok","timestamp":1728552208339,"user_tz":-180,"elapsed":353,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Create Gradio interface\n","iface = gr.Interface(\n","    fn=audio_sentiment_analyzer,\n","    inputs=gr.Audio(type=\"numpy\"),  # Adjusted the input here\n","    outputs=[\n","        gr.Textbox(label=\"Transcription\"),\n","        gr.Textbox(label=\"Sentiment\"),\n","        gr.Plot(label=\"Audio Waveform\"),\n","        gr.Plot(label=\"Audio Features\")\n","    ],\n","    title=\"Audio Sentiment Classifier\",\n","    description=\"Record audio to analyze its sentiment and transcribe the speech.\",\n","    theme=\"huggingface\",\n","    allow_flagging=\"never\"\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4kphtZvlbaN","executionInfo":{"status":"ok","timestamp":1728552211445,"user_tz":-180,"elapsed":665,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}},"outputId":"a8881d42-f709-4a19-aba3-cf4131bc5a1b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1027: UserWarning: Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-67079d12-05d813c11d19492b4426ea93;356ffe6a-35d6-4f03-803a-f577326f070d)\n","\n","Sorry, we can't find the page you are looking for.\n","  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n","/usr/local/lib/python3.10/dist-packages/gradio/interface.py:393: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Launch Gradio interface\n","# Click the link, record using the browser, submit\n","iface.launch(debug=True, share=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"PtZUG3TxlbWn","executionInfo":{"status":"ok","timestamp":1728552282168,"user_tz":-180,"elapsed":68142,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}},"outputId":"fb5b1e04-aa27-44b5-d3ed-603dfed17f99"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://1642ec8f7fc651af24.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://1642ec8f7fc651af24.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-28-cfdc0d19e6c2>:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  return float(pitch), float(tempo), float(tone)  # Ensure these are floats\n"]},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://1642ec8f7fc651af24.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["# Push to github\n"],"metadata":{"id":"zaS4AZy-tNap"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"5bNu8XXzlbTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/ai_app_development\n"],"metadata":{"id":"DhIS0ojblbQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git init"],"metadata":{"id":"LN59vqxFnLPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email \"mohan_gi@hotmail.com\"\n","!git config --global user.name \"girimohan\""],"metadata":{"id":"s_zF70y5nLYC","executionInfo":{"status":"ok","timestamp":1728550396328,"user_tz":-180,"elapsed":509,"user":{"displayName":"Mohan Giri","userId":"14738855864626443163"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["!git add .\n","!git commit -m \"Initial commit with project files\""],"metadata":{"id":"22EYSXWgofts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Push the changes again\n","!git push -f origin main"],"metadata":{"id":"_GPmWQ4UpQSc"},"execution_count":null,"outputs":[]}]}